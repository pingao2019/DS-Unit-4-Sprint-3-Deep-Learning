{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.23.1"
    },
    "colab": {
      "name": "LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZsic4Oz4J20",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
        "# Convolutional Neural Networks (CNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0lfZdD_cp1t5"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
        "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
        "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
        "\n",
        "\n",
        "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "|Mountain (+)|Forest (-)|\n",
        "|---|---|\n",
        "|![](https://github.com/pingao2019/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module2-convolutional-neural-networks/data/train/mountain/art1131.jpg?raw=1)|![](https://github.com/pingao2019/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module2-convolutional-neural-networks/data/validation/forest/cdmc317.jpg?raw=1)|\n",
        "\n",
        "The problem is relatively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several different possible models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZBqu-zfR4J26"
      },
      "source": [
        "# Pre - Trained Model\n",
        "<a id=\"p1\"></a>\n",
        "\n",
        "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model # This is the functional API\n",
        "\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "```\n",
        "\n",
        "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
        "\n",
        "```python\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
        "\n",
        "```python\n",
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(resnet.input, predictions)\n",
        "```\n",
        "\n",
        "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n",
        "\n",
        "Steps to complete assignment: \n",
        "1. Load in Image Data into numpy arrays (`X`) \n",
        "2. Create a `y` for the labels\n",
        "3. Train your model with pre-trained layers from resnet\n",
        "4. Report your model's accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aIfMm8B4J29",
        "colab_type": "text"
      },
      "source": [
        "## Load in Data\n",
        "\n",
        "This surprisingly more difficult than it seems, because you are working with directories of images instead of a single file. This boiler plate will help you download a zipped version of the directory of images. The directory is organized into \"train\" and \"validation\" which you can use inside an `ImageGenerator` class to stream batches of images thru your model.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21c7P7D44J3A",
        "colab_type": "text"
      },
      "source": [
        "### Download & Summarize the Data\n",
        "\n",
        "This step is completed for you. Just run the cells and review the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBOg6urs4J3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "_URL = 'https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module2-convolutional-neural-networks/data.zip?raw=true'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('./data.zip', origin=_URL, extract=True)\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "get5Kfgn4J3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWoZ4neQ4J3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mountain_dir = os.path.join(train_dir, 'mountain')  # directory with our training cat pictures\n",
        "train_forest_dir = os.path.join(train_dir, 'forest')  # directory with our training dog pictures\n",
        "validation_mountain_dir = os.path.join(validation_dir, 'mountain')  # directory with our validation cat pictures\n",
        "validation_forest_dir = os.path.join(validation_dir, 'forest')  # directory with our validation dog pictures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WLijhGC4J3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_mountain_tr = len(os.listdir(train_mountain_dir))\n",
        "num_forest_tr = len(os.listdir(train_forest_dir))\n",
        "\n",
        "num_mountain_val = len(os.listdir(validation_mountain_dir))\n",
        "num_forest_val = len(os.listdir(validation_forest_dir))\n",
        "\n",
        "total_train = num_mountain_tr + num_forest_tr\n",
        "total_val = num_mountain_val + num_forest_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P26GtuFW4J3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "3f7bc978-19be-468d-e96c-c384d7d4e20e"
      },
      "source": [
        "print('total training mountain images:', num_mountain_tr)\n",
        "print('total training forest images:', num_forest_tr)\n",
        "\n",
        "print('total validation mountain images:', num_mountain_val)\n",
        "print('total validation forest images:', num_forest_val)\n",
        "print(\"--\")\n",
        "print(\"Total training images:\", total_train)\n",
        "print(\"Total validation images:\", total_val)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training mountain images: 254\n",
            "total training forest images: 270\n",
            "total validation mountain images: 125\n",
            "total validation forest images: 62\n",
            "--\n",
            "Total training images: 524\n",
            "Total validation images: 187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMmAxd_04J4B",
        "colab_type": "text"
      },
      "source": [
        "### Keras `ImageGenerator` to Process the Data\n",
        "\n",
        "This step is completed for you, but please review the code. The `ImageGenerator` class reads in batches of data from a directory and pass them to the model one batch at a time. Just like large text files, this method is advantageous, because it stifles the need to load a bunch of images into memory. \n",
        "\n",
        "Check out the documentation for this class method: [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). You'll expand it's use in the third assignment objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEvjxDwu4J4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7KZSUFZ4J4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "epochs = 20\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht1vnXlG4J4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3acfcd78-6996-4aaf-c0f5-d84d3e6294d7"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 533 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bmb6QcCn4J4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2422b02f-7d42-467a-844c-4ebee3ce3fee"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWnTyUW4J42",
        "colab_type": "text"
      },
      "source": [
        "## Instatiate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGdK6hiAa4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50 #for Pre - Trained Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        " \n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model  \n",
        " #exclude all dense layers for include_top=False,i.e., ouput layer\n",
        "resnet = ResNet50(weights='imagenet', include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YtbsVpHAvvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#feature extration is done, turn off updating the weights \n",
        "#because resnet has finished feature extraction and trained image well\n",
        "\n",
        "for layer in resnet.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Function API\n",
        "x = resnet.output\n",
        "x = GlobalAveragePooling2D()(x) # This convert all converlution layers into a giant flatten\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x) #ouput layer\n",
        "\n",
        "model = Model(resnet.input, predictions)\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf7hdyGSFroP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WzzD0fr4J5G",
        "colab_type": "text"
      },
      "source": [
        "## Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7WzJzB_4J5K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "0214c433-4cc4-43ce-b844-cb5e383c802b"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5036 - accuracy: 0.7844 - val_loss: 0.5066 - val_accuracy: 0.8580\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6441 - accuracy: 0.8224 - val_loss: 0.6501 - val_accuracy: 0.6875\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5170 - accuracy: 0.7745 - val_loss: 0.4804 - val_accuracy: 0.8977\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.5611 - accuracy: 0.7745 - val_loss: 0.4813 - val_accuracy: 0.8977\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5115 - accuracy: 0.8066 - val_loss: 0.5240 - val_accuracy: 0.9034\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.5210 - accuracy: 0.7725 - val_loss: 0.5165 - val_accuracy: 0.8693\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.6731 - accuracy: 0.7984 - val_loss: 0.4506 - val_accuracy: 0.9091\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.4756 - accuracy: 0.8164 - val_loss: 0.4567 - val_accuracy: 0.8920\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.4497 - accuracy: 0.8124 - val_loss: 0.6608 - val_accuracy: 0.8523\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.5588 - accuracy: 0.7705 - val_loss: 0.5030 - val_accuracy: 0.9091\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.7087 - accuracy: 0.8044 - val_loss: 0.6218 - val_accuracy: 0.8864\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.4668 - accuracy: 0.8184 - val_loss: 0.4206 - val_accuracy: 0.9091\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5799 - accuracy: 0.7984 - val_loss: 0.4638 - val_accuracy: 0.8977\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.4549 - accuracy: 0.8144 - val_loss: 1.1340 - val_accuracy: 0.7955\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5817 - accuracy: 0.8024 - val_loss: 0.4593 - val_accuracy: 0.8920\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.7893 - accuracy: 0.8303 - val_loss: 0.2991 - val_accuracy: 0.8750\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.4775 - accuracy: 0.8323 - val_loss: 0.2433 - val_accuracy: 0.9148\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.4898 - accuracy: 0.8104 - val_loss: 0.3391 - val_accuracy: 0.8750\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.4912 - accuracy: 0.8244 - val_loss: 0.4071 - val_accuracy: 0.9318\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.7000 - accuracy: 0.7525 - val_loss: 0.4180 - val_accuracy: 0.8807\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9apcAzEI4J5X",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model\n",
        "\n",
        "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIpEtdM_MInw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "83d4d106-f14d-4797-9231-15d9c29422db"
      },
      "source": [
        "# Define the Model\n",
        "from tensorflow.keras.models import Sequential, Model \n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "\n",
        "\n",
        "model= Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='selu', input_shape=(256,256,3)))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='selu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Conv2D(64, (3,3), activation='selu'))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.15))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 254, 254, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 127, 127, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 62, 62, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 60, 60, 64)        36928     \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_12  (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 60,545\n",
            "Trainable params: 60,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3b8Frm_MwVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhRpyay8M0f8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "abdcbafa-c8f2-4064-99a0-78a5eb84301d"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.4095 - accuracy: 0.8443 - val_loss: 0.2081 - val_accuracy: 0.9318\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.2295 - accuracy: 0.9361 - val_loss: 0.3492 - val_accuracy: 0.8466\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.2163 - accuracy: 0.9222 - val_loss: 0.1730 - val_accuracy: 0.9318\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.1997 - accuracy: 0.9301 - val_loss: 0.2319 - val_accuracy: 0.9091\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.1612 - accuracy: 0.9441 - val_loss: 0.2099 - val_accuracy: 0.8977\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.2044 - accuracy: 0.9222 - val_loss: 0.2849 - val_accuracy: 0.8864\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.1781 - accuracy: 0.9281 - val_loss: 0.1875 - val_accuracy: 0.9261\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.2047 - accuracy: 0.9261 - val_loss: 0.2111 - val_accuracy: 0.8977\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1499 - accuracy: 0.9461 - val_loss: 0.2476 - val_accuracy: 0.9148\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.1607 - accuracy: 0.9501 - val_loss: 0.2276 - val_accuracy: 0.9034\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1288 - accuracy: 0.9521 - val_loss: 0.3483 - val_accuracy: 0.8864\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1601 - accuracy: 0.9301 - val_loss: 0.2376 - val_accuracy: 0.9148\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1675 - accuracy: 0.9421 - val_loss: 0.2275 - val_accuracy: 0.9261\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1679 - accuracy: 0.9341 - val_loss: 0.1911 - val_accuracy: 0.9261\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1995 - accuracy: 0.9301 - val_loss: 0.1877 - val_accuracy: 0.9318\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1838 - accuracy: 0.9321 - val_loss: 0.2074 - val_accuracy: 0.9205\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1497 - accuracy: 0.9531 - val_loss: 0.1978 - val_accuracy: 0.9375\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 44s 1s/step - loss: 0.1365 - accuracy: 0.9561 - val_loss: 0.2654 - val_accuracy: 0.9205\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1270 - accuracy: 0.9501 - val_loss: 0.2093 - val_accuracy: 0.9318\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 45s 1s/step - loss: 0.1283 - accuracy: 0.9581 - val_loss: 0.2496 - val_accuracy: 0.9205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZZmj_3kmF8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0625ef23-21c0-4e6a-fc95-d553d8620cd0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show();"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rX3-vO4J6Q",
        "colab_type": "text"
      },
      "source": [
        "# Custom CNN Model with Image Manipulations\n",
        "\n",
        "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Simply, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n",
        "\n",
        "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whEqu51j02U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_image_generator = ImageDataGenerator(\n",
        "                      rotation_range=40,\n",
        "                      width_shift_range=0.2,\n",
        "                      height_shift_range=0.2,\n",
        "                      shear_range=0.2,\n",
        "                      zoom_range=0.2,\n",
        "                      horizontal_flip=True,\n",
        "                      vertical_flip=True,\n",
        "                      fill_mode='nearest')\n",
        "validation_image_generator = ImageDataGenerator(\n",
        "                           rotation_range=40,\n",
        "                           width_shift_range=0.2,\n",
        "                           height_shift_range=0.2,\n",
        "                           shear_range=0.2,\n",
        "                           zoom_range=0.2,\n",
        "                           horizontal_flip=True,\n",
        "                           vertical_flip=True,\n",
        "                           fill_mode='nearest')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8tkcvIf1MCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9f24271f-8853-4734-8098-55f0814d0866"
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 533 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGOmx0lB1Pqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "03e6dcb8-8ed6-45c0-e9ad-13c58ffb50f2"
      },
      "source": [
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 195 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqwPCjMw1UN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "e8d4e835-c633-4054-a34a-a5b80c8a76fe"
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 111, 111, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 109, 109, 32)      9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 54, 54, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 52, 52, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 26, 26, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 36864)             0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 64)                2359360   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,424,993\n",
            "Trainable params: 2,424,993\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO5pud0i1ZiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "656245ee-647e-444e-e5ef-d8182a2234d2"
      },
      "source": [
        "# Compile Model\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit Model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 13.9852 - accuracy: 0.5669 - val_loss: 7.5277 - val_accuracy: 0.3523\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.9457 - accuracy: 0.5729 - val_loss: 0.7223 - val_accuracy: 0.3295\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 4.1920 - accuracy: 0.5010 - val_loss: 0.6559 - val_accuracy: 0.3239\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 1.7656 - accuracy: 0.5749 - val_loss: 0.6557 - val_accuracy: 0.8466\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6319 - accuracy: 0.7285 - val_loss: 0.5943 - val_accuracy: 0.8239\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.9087 - accuracy: 0.7325 - val_loss: 1.9902 - val_accuracy: 0.3977\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 0.6248 - accuracy: 0.7345 - val_loss: 0.5678 - val_accuracy: 0.8295\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6079 - accuracy: 0.6562 - val_loss: 0.6620 - val_accuracy: 0.7045\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 39s 1s/step - loss: 1.0196 - accuracy: 0.7246 - val_loss: 0.5855 - val_accuracy: 0.8352\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.8071 - accuracy: 0.6028 - val_loss: 0.6677 - val_accuracy: 0.6875\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.8537 - accuracy: 0.6307 - val_loss: 0.5850 - val_accuracy: 0.8409\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5734 - accuracy: 0.7445 - val_loss: 0.6578 - val_accuracy: 0.8352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6025 - accuracy: 0.7146 - val_loss: 0.6591 - val_accuracy: 0.8693\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6039 - accuracy: 0.7525 - val_loss: 0.6223 - val_accuracy: 0.7614\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.8605 - accuracy: 0.7505 - val_loss: 0.5514 - val_accuracy: 0.8807\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5360 - accuracy: 0.7625 - val_loss: 0.6088 - val_accuracy: 0.7784\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5663 - accuracy: 0.7285 - val_loss: 0.5854 - val_accuracy: 0.7955\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.5972 - accuracy: 0.7166 - val_loss: 0.5282 - val_accuracy: 0.8807\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6535 - accuracy: 0.7924 - val_loss: 0.4917 - val_accuracy: 0.8523\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 40s 1s/step - loss: 0.6473 - accuracy: 0.8004 - val_loss: 0.5669 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ENBHZ8OK4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "623e4143-6bad-49b9-d770-4e9943cd8c61"
      },
      "source": [
        "train_data_gen.image_shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd4z1VTIOSBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_count = range(1, len(augmented.history['loss']) + 1)\n",
        "\n",
        "plt.plot(epoch_count , augmented.history['loss'], 'p-')\n",
        "plt.plot(epoch_count , augmented.history['val_loss'], 'cyan')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.show();\n",
        "\n",
        "epoch_count = range(1, len(augmented.history['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epoch_count , augmented.history['accuracy'], 'p-')\n",
        "plt.plot(epoch_count , augmented.history['val_accuracy'], 'cyan')\n",
        "plt.legend(['Training Acc', 'Validation Acc'])\n",
        "plt.show();\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "# Resources and Stretch Goals\n",
        "\n",
        "Stretch goals\n",
        "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
        "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
        "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
        "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
        "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
        "\n",
        "Resources\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
        "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
        "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
        "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
        "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
      ]
    }
  ]
}